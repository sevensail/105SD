{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TrumpTalks.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 2",
      "language": "python",
      "name": "python2"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "OJy9HNwvM7Dn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Case Study 1 : Data Science in Twitter Data"
      ]
    },
    {
      "metadata": {
        "id": "Klq1scPpM7Dr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Required Readings:** \n",
        "* Chapter 1 and Chapter 9 of the book [Mining the Social Web](http://www.webpages.uidaho.edu/~stevel/504/Mining-the-Social-Web-2nd-Edition.pdf) \n",
        "* The codes for [Chapter 1](http://bit.ly/1qCtMrr) and [Chapter 9](http://bit.ly/1u7eP33)\n",
        "* [TED Talks](https://www.ted.com/talks) for examples of 10 minutes talks.\n",
        "\n",
        "** NOTE **\n",
        "* Please don't forget to save the notebook frequently when working in Jupyter Notebook, otherwise the changes you made can be lost.\n",
        "\n",
        "*----------------------"
      ]
    },
    {
      "metadata": {
        "id": "zqKosDALM7Dt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Problem: pick a data science problem that you plan to solve using Twitter Data\n",
        "* The problem should be important and interesting, which has a potential impact in some area.\n",
        "* The problem should be solvable using twitter data and data science solutions.\n",
        "\n",
        "Please briefly describe in the following cell: what problem are you trying to solve? why this problem is important and interesting?\n"
      ]
    },
    {
      "metadata": {
        "id": "857hIul5JIjh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We are interested in the potential impact a person of power's inner thoughts can have on the world at large.  In particular, we are interested at what are the potential economic impacts and ramifcations of the tweets that the current acting president of the United States posts.  In other words, we want to analyze impact, if any, Trump's tweets have on the stock market.\n",
        "\n",
        "This problem is interesting, as, in general, it could potentially be used to rank the economical importance of any particular individual (that uses Twitter)."
      ]
    },
    {
      "metadata": {
        "id": "hTrSNFbSM7D2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Data Collection: Download Twitter Data using API"
      ]
    },
    {
      "metadata": {
        "id": "8O9SeMBzM7D3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "* In order to solve the above problem, you need to collect some twitter data. You could select a topic that is relevant to your problem, and use Twitter API to download the relevant tweets. It is recommended that the number of tweets should be larger than 200, but smaller than 1 million.\n",
        "* Store the tweets you downloaded into a local file (txt file or json file) "
      ]
    },
    {
      "metadata": {
        "id": "LWZJo-wsNJJg",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 28
            },
            {
              "item_id": 42
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 1415
        },
        "outputId": "d66a2c82-e083-46c6-ce46-77050d6c348d",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1518034749230,
          "user_tz": 300,
          "elapsed": 26887,
          "user": {
            "displayName": "Mihin Sumaria",
            "photoUrl": "//lh4.googleusercontent.com/-QbENF0Rj_SU/AAAAAAAAAAI/AAAAAAAAG0E/mqCCHfZsJ58/s50-c-k-no/photo.jpg",
            "userId": "111360066517566617715"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Install dependencies on the google-provided VM hosting this notebook\n",
        "# Only needs to be run once at the top of the notebook.\n",
        "!pip install twitter\n",
        "!pip install plotly\n",
        "!pip install textblob\n",
        "!pip install prettytable\n",
        "!pip install quandl\n",
        "!pip install pandas\n",
        "!pip install python-dateutil\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting twitter\n",
            "  Downloading twitter-1.18.0-py2.py3-none-any.whl (54kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 2.3MB/s \n",
            "\u001b[?25hInstalling collected packages: twitter\n",
            "Successfully installed twitter-1.18.0\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python2.7/dist-packages\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python2.7/dist-packages (from plotly)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python2.7/dist-packages (from plotly)\n",
            "Requirement already satisfied: six in /usr/local/lib/python2.7/dist-packages (from plotly)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python2.7/dist-packages (from requests->plotly)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python2.7/dist-packages (from requests->plotly)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python2.7/dist-packages (from requests->plotly)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python2.7/dist-packages (from requests->plotly)\n",
            "Collecting textblob\n",
            "  Downloading textblob-0.15.1-py2.py3-none-any.whl (631kB)\n",
            "\u001b[K    100% |████████████████████████████████| 634kB 1.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: nltk>=3.1 in /usr/local/lib/python2.7/dist-packages (from textblob)\n",
            "Installing collected packages: textblob\n",
            "Successfully installed textblob-0.15.1\n",
            "Collecting prettytable\n",
            "  Downloading prettytable-0.7.2.zip\n",
            "Building wheels for collected packages: prettytable\n",
            "  Running setup.py bdist_wheel for prettytable ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/b6/90/7b/1c22b89217d0eba6d5f406e562365ebee804f0d4595b2bdbcd\n",
            "Successfully built prettytable\n",
            "Installing collected packages: prettytable\n",
            "Successfully installed prettytable-0.7.2\n",
            "Collecting quandl\n",
            "  Downloading Quandl-3.3.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python2.7/dist-packages (from quandl)\n",
            "Requirement already satisfied: pyasn1 in /usr/local/lib/python2.7/dist-packages (from quandl)\n",
            "Collecting pyOpenSSL (from quandl)\n",
            "  Downloading pyOpenSSL-17.5.0-py2.py3-none-any.whl (53kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 3.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.8 in /usr/local/lib/python2.7/dist-packages (from quandl)\n",
            "Collecting more-itertools (from quandl)\n",
            "  Downloading more_itertools-4.1.0-py2-none-any.whl (47kB)\n",
            "\u001b[K    100% |████████████████████████████████| 51kB 6.6MB/s \n",
            "\u001b[?25hCollecting ndg-httpsclient (from quandl)\n",
            "  Downloading ndg_httpsclient-0.4.4-py2-none-any.whl\n",
            "Requirement already satisfied: pandas>=0.14 in /usr/local/lib/python2.7/dist-packages (from quandl)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python2.7/dist-packages (from quandl)\n",
            "Collecting inflection>=0.3.1 (from quandl)\n",
            "  Downloading inflection-0.3.1.tar.gz\n",
            "Requirement already satisfied: requests>=2.7.0 in /usr/local/lib/python2.7/dist-packages (from quandl)\n",
            "Collecting cryptography>=2.1.4 (from pyOpenSSL->quandl)\n",
            "  Downloading cryptography-2.1.4-cp27-cp27mu-manylinux1_x86_64.whl (2.2MB)\n",
            "\u001b[K    100% |████████████████████████████████| 2.2MB 457kB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz>=2011k in /usr/local/lib/python2.7/dist-packages (from pandas>=0.14->quandl)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python2.7/dist-packages (from requests>=2.7.0->quandl)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python2.7/dist-packages (from requests>=2.7.0->quandl)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python2.7/dist-packages (from requests>=2.7.0->quandl)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python2.7/dist-packages (from requests>=2.7.0->quandl)\n",
            "Collecting cffi>=1.7; platform_python_implementation != \"PyPy\" (from cryptography>=2.1.4->pyOpenSSL->quandl)\n",
            "  Downloading cffi-1.11.4-cp27-cp27mu-manylinux1_x86_64.whl (406kB)\n",
            "\u001b[K    100% |████████████████████████████████| 409kB 2.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: enum34; python_version < \"3\" in /usr/local/lib/python2.7/dist-packages (from cryptography>=2.1.4->pyOpenSSL->quandl)\n",
            "Collecting asn1crypto>=0.21.0 (from cryptography>=2.1.4->pyOpenSSL->quandl)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  Downloading asn1crypto-0.24.0-py2.py3-none-any.whl (101kB)\r\n",
            "\u001b[K    100% |████████████████████████████████| 102kB 9.1MB/s \n",
            "\u001b[?25hCollecting ipaddress; python_version < \"3\" (from cryptography>=2.1.4->pyOpenSSL->quandl)\n",
            "  Downloading ipaddress-1.0.19.tar.gz\n",
            "Collecting pycparser (from cffi>=1.7; platform_python_implementation != \"PyPy\"->cryptography>=2.1.4->pyOpenSSL->quandl)\n",
            "  Downloading pycparser-2.18.tar.gz (245kB)\n",
            "\u001b[K    100% |████████████████████████████████| 256kB 4.1MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: inflection, ipaddress, pycparser\n",
            "  Running setup.py bdist_wheel for inflection ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/41/fa/e9/2995f4ab121e9f30f342fa2d43f0b27f851a0cb9f0d98d3b45\n",
            "  Running setup.py bdist_wheel for ipaddress ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/d7/6b/69/666188e8101897abb2e115d408d139a372bdf6bfa7abb5aef5\n",
            "  Running setup.py bdist_wheel for pycparser ... \u001b[?25l-\b \b\\\b \bdone\n",
            "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/95/14/9a/5e7b9024459d2a6600aaa64e0ba485325aff7a9ac7489db1b6\n",
            "Successfully built inflection ipaddress pycparser\n",
            "Installing collected packages: pycparser, cffi, asn1crypto, ipaddress, cryptography, pyOpenSSL, more-itertools, ndg-httpsclient, inflection, quandl\n",
            "Successfully installed asn1crypto-0.24.0 cffi-1.11.4 cryptography-2.1.4 inflection-0.3.1 ipaddress-1.0.19 more-itertools-4.1.0 ndg-httpsclient-0.4.4 pyOpenSSL-17.5.0 pycparser-2.18 quandl-3.3.0\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python2.7/dist-packages\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python2.7/dist-packages (from pandas)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python2.7/dist-packages (from pandas)\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python2.7/dist-packages (from pandas)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python2.7/dist-packages (from python-dateutil->pandas)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python2.7/dist-packages\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python2.7/dist-packages (from python-dateutil)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Nya0yC2GM7D4",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Define a bunch of helper functions ripped from chapter's 1 && 9 of\n",
        "# \"Mining the Social Web\"\n",
        "\n",
        "import twitter\n",
        "# Define a Function to Login Twitter API\n",
        "def oauth_login():\n",
        "    # Go to http://twitter.com/apps/new to create an app and get values\n",
        "    # for these credentials that you'll need to provide in place of these\n",
        "    # empty string values that are defined as placeholders.\n",
        "    # See https://dev.twitter.com/docs/auth/oauth for more information \n",
        "    # on Twitter's OAuth implementation.\n",
        "    \n",
        "    CONSUMER_KEY = 'ouMDjeSY1kY9hqUDuN3WYGxEJ'\n",
        "    CONSUMER_SECRET ='Gzw5aljoe0cONRhof5mQ0bbSvqpUjyzUiGDBe5Lfoskzki7xUe'\n",
        "    OAUTH_TOKEN = '958500278151602176-jTycQLTNHoR3CEUYz5rJOUg6w7VSB7M'\n",
        "    OAUTH_TOKEN_SECRET = '7quNIFQRGI29WZ1ba3UXDSNGQwuQf8MQo8JRVEwdhU8lI'\n",
        "    \n",
        "    auth = twitter.oauth.OAuth(OAUTH_TOKEN, OAUTH_TOKEN_SECRET,\n",
        "                               CONSUMER_KEY, CONSUMER_SECRET)\n",
        "    \n",
        "    twitter_api = twitter.Twitter(auth=auth)\n",
        "    return twitter_api\n",
        "  \n",
        "#------------------------------------------------------------------------------\n",
        "import sys\n",
        "import time\n",
        "from urllib2 import URLError\n",
        "from httplib import BadStatusLine\n",
        "import json\n",
        "import twitter\n",
        "\n",
        "def make_twitter_request(twitter_api_func, max_errors=10, *args, **kw): \n",
        "    # A nested helper function that handles common HTTPErrors. Return an updated\n",
        "    # value for wait_period if the problem is a 500 level error. Block until the\n",
        "    # rate limit is reset if it's a rate limiting issue (429 error). Returns None\n",
        "    # for 401 and 404 errors, which requires special handling by the caller.\n",
        "    def handle_twitter_http_error(e, wait_period=2, sleep_when_rate_limited=True):\n",
        "    \n",
        "        if wait_period > 3600: # Seconds\n",
        "            print >> sys.stderr, 'Too many retries. Quitting.'\n",
        "            raise e\n",
        "    \n",
        "        # See https://dev.twitter.com/docs/error-codes-responses for common codes\n",
        "    \n",
        "        if e.e.code == 401:\n",
        "            print >> sys.stderr, 'Encountered 401 Error (Not Authorized)'\n",
        "            return None\n",
        "        elif e.e.code == 404:\n",
        "            print >> sys.stderr, 'Encountered 404 Error (Not Found)'\n",
        "            return None\n",
        "        elif e.e.code == 429: \n",
        "            print >> sys.stderr, 'Encountered 429 Error (Rate Limit Exceeded)'\n",
        "            if sleep_when_rate_limited:\n",
        "                print >> sys.stderr, \"Retrying in 15 minutes...ZzZ...\"\n",
        "                sys.stderr.flush()\n",
        "                time.sleep(60*15 + 5)\n",
        "                print >> sys.stderr, '...ZzZ...Awake now and trying again.'\n",
        "                return 2\n",
        "            else:\n",
        "                raise e # Caller must handle the rate limiting issue\n",
        "        elif e.e.code in (500, 502, 503, 504):\n",
        "            print >> sys.stderr, 'Encountered %i Error. Retrying in %i seconds' % \\\n",
        "                (e.e.code, wait_period)\n",
        "            time.sleep(wait_period)\n",
        "            wait_period *= 1.5\n",
        "            return wait_period\n",
        "        else:\n",
        "            raise e\n",
        "\n",
        "    # End of nested helper function\n",
        "    \n",
        "    wait_period = 2 \n",
        "    error_count = 0 \n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            return twitter_api_func(*args, **kw)\n",
        "        except twitter.api.TwitterHTTPError, e:\n",
        "            error_count = 0 \n",
        "            wait_period = handle_twitter_http_error(e, wait_period)\n",
        "            if wait_period is None:\n",
        "                return\n",
        "        except URLError, e:\n",
        "            error_count += 1\n",
        "            time.sleep(wait_period)\n",
        "            wait_period *= 1.5\n",
        "            print >> sys.stderr, \"URLError encountered. Continuing.\"\n",
        "            if error_count > max_errors:\n",
        "                print >> sys.stderr, \"Too many consecutive errors...bailing out.\"\n",
        "                raise\n",
        "        except BadStatusLine, e:\n",
        "            error_count += 1\n",
        "            time.sleep(wait_period)\n",
        "            wait_period *= 1.5\n",
        "            print >> sys.stderr, \"BadStatusLine encountered. Continuing.\"\n",
        "            if error_count > max_errors:\n",
        "                print >> sys.stderr, \"Too many consecutive errors...bailing out.\"\n",
        "                raise\n",
        "                \n",
        "#------------------------------------------------------------------------------\n",
        "def harvest_user_timeline(twitter_api, screen_name=None, user_id=None, max_results=1000):\n",
        "    assert (screen_name != None) != (user_id != None), \\\n",
        "    \"Must have screen_name or user_id, but not both\"    \n",
        "    \n",
        "    kw = {  # Keyword args for the Twitter API call\n",
        "        'count': 200,\n",
        "        'trim_user': 'true',\n",
        "        'include_rts' : 'true',\n",
        "        'since_id' : 822501803615014918,\n",
        "        'tweet_mode' : 'extended'\n",
        "        }\n",
        "    \n",
        "    if screen_name:\n",
        "        kw['screen_name'] = screen_name\n",
        "    else:\n",
        "        kw['user_id'] = user_id\n",
        "        \n",
        "    max_pages = 16\n",
        "    results = []\n",
        "    \n",
        "    tweets = make_twitter_request(twitter_api.statuses.user_timeline, **kw)\n",
        "    \n",
        "    if tweets is None: # 401 (Not Authorized) - Need to bail out on loop entry\n",
        "        tweets = []\n",
        "        \n",
        "    results += tweets\n",
        "    \n",
        "    print >> sys.stderr, 'Fetched %i tweets' % len(tweets)\n",
        "    \n",
        "    page_num = 1\n",
        "    \n",
        "    # Many Twitter accounts have fewer than 200 tweets so you don't want to enter\n",
        "    # the loop and waste a precious request if max_results = 200.\n",
        "    \n",
        "    # Note: Analogous optimizations could be applied inside the loop to try and \n",
        "    # save requests. e.g. Don't make a third request if you have 287 tweets out of \n",
        "    # a possible 400 tweets after your second request. Twitter does do some \n",
        "    # post-filtering on censored and deleted tweets out of batches of 'count', though,\n",
        "    # so you can't strictly check for the number of results being 200. You might get\n",
        "    # back 198, for example, and still have many more tweets to go. If you have the\n",
        "    # total number of tweets for an account (by GET /users/lookup/), then you could \n",
        "    # simply use this value as a guide.\n",
        "    \n",
        "    if max_results == kw['count']:\n",
        "        page_num = max_pages # Prevent loop entry\n",
        "    \n",
        "    while page_num < max_pages and len(tweets) > 0 and len(results) < max_results:\n",
        "    \n",
        "        # Necessary for traversing the timeline in Twitter's v1.1 API:\n",
        "        # get the next query's max-id parameter to pass in.\n",
        "        # See https://dev.twitter.com/docs/working-with-timelines.\n",
        "        kw['max_id'] = min([ tweet['id'] for tweet in tweets]) - 1 \n",
        "    \n",
        "        tweets = make_twitter_request(twitter_api.statuses.user_timeline, **kw)\n",
        "        results += tweets\n",
        "\n",
        "        print >> sys.stderr, 'Fetched %i tweets' % (len(tweets),)\n",
        "    \n",
        "        page_num += 1\n",
        "        \n",
        "    print >> sys.stderr, 'Done fetching tweets'\n",
        "\n",
        "    return results[:max_results]\n",
        "  \n",
        "#------------------------------------------------------------------------------\n",
        "import re\n",
        "\n",
        "def clean_tweet(tweet):\n",
        "    '''\n",
        "    Utility function to clean the text in a tweet by removing \n",
        "    links and special characters using regex.\n",
        "    '''\n",
        "    return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", tweet).split())\n",
        "  \n",
        "  \n",
        "#------------------------------------------------------------------------------\n",
        "# set up plotly API\n",
        "import plotly\n",
        "\n",
        "plotly.tools.set_credentials_file(username='sevensail', api_key='ikeKkHpLj086xlhPIkVm')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "62OU2ZeQM7ET",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Collect Tweets and rebort basic statistics"
      ]
    },
    {
      "metadata": {
        "id": "s6AvTze9M7EV",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 13
            },
            {
              "item_id": 14
            },
            {
              "item_id": 15
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 303
        },
        "outputId": "44b6f37a-0cc6-4d37-c0ad-effecf0fc6e1",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1518034877589,
          "user_tz": 300,
          "elapsed": 7538,
          "user": {
            "displayName": "Mihin Sumaria",
            "photoUrl": "//lh4.googleusercontent.com/-QbENF0Rj_SU/AAAAAAAAAAI/AAAAAAAAG0E/mqCCHfZsJ58/s50-c-k-no/photo.jpg",
            "userId": "111360066517566617715"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# The total number of tweets collected:  1000\n",
        "twitter_api = oauth_login()\n",
        "tweets = harvest_user_timeline(twitter_api, screen_name=\"realDonaldTrump\", \\\n",
        "                               max_results=3200)\n",
        "\n",
        "print \"Successfully Collected: \" + str(len(tweets)) + \" tweets\"\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fetched 200 tweets\n",
            "Fetched 200 tweets\n",
            "Fetched 197 tweets\n",
            "Fetched 200 tweets\n",
            "Fetched 199 tweets\n",
            "Fetched 200 tweets\n",
            "Fetched 200 tweets\n",
            "Fetched 200 tweets\n",
            "Fetched 200 tweets\n",
            "Fetched 200 tweets\n",
            "Fetched 200 tweets\n",
            "Fetched 200 tweets\n",
            "Fetched 200 tweets\n",
            "Fetched 28 tweets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Successfully Collected: 2624 tweets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Fetched 0 tweets\n",
            "Done fetching tweets\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "1Dosvad0M7Eb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Data Exploration: Exploring the Tweets and Tweet Entities\n",
        "\n",
        "**(1) Word Count:** \n",
        "* Load the tweets you collected in the local file (txt or json)\n",
        "* compute the frequencies of the words being used in these tweets. \n",
        "* Plot a table of the top 30 most-frequent words with their counts"
      ]
    },
    {
      "metadata": {
        "id": "3d0_33KkM7Eg",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 1
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 546
        },
        "outputId": "03443c37-5613-4c54-aec3-840559c2a579",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1517942789526,
          "user_tz": 300,
          "elapsed": 1947,
          "user": {
            "displayName": "tian xie",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "108849885565270907176"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Your code starts here\n",
        "#   Please add comments or text cells in between to explain the general idea of each block of the code.\n",
        "#   Please feel free to add more cells below this cell if necessary\n",
        "\n",
        "tweets_text = [clean_tweet(tweet['full_text']) for tweet in tweets ]\n",
        "words = [w.lower() for t in tweets_text for w in t.split()]\n",
        "\n",
        "from collections import Counter\n",
        "c = Counter(words)\n",
        "useless = ['the','to','of','and','a','in','for','is','our','are','on','that','i','with','was','it','we','my','be','has','you','as','at','s']\n",
        "frequency = c.most_common()[:(50+len(useless))]\n",
        "w = [tuple[0] for tuple in frequency if tuple[0] not in useless]\n",
        "f = [tuple[1] for tuple in frequency if tuple[0] not in useless]\n",
        "\n",
        "import plotly.plotly as py\n",
        "from plotly.graph_objs import *\n",
        "\n",
        "data = Data([Bar(x=w,y=f)])\n",
        "layout = Layout(title = \"30 Most Frequent Words\", xaxis = {'title':'Words'}, yaxis = {'title':'Frequency'})\n",
        "figure = Figure(data = data, layout = layout)\n",
        "py.iplot(figure)\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~sevensail/28.embed\" height=\"525px\" width=\"100%\"></iframe>"
            ],
            "text/plain": [
              "<plotly.tools.PlotlyDisplay object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "metadata": {
        "id": "oFYsc0bNM7En",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "** (2) Find the most popular tweets in your collection of tweets**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "Please plot a table of the top 10 most-retweeted tweets in your collection, i.e., the tweets with the largest number of retweet counts.\n"
      ]
    },
    {
      "metadata": {
        "id": "d5OPp04IM7Ep",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 1
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "outputId": "961c36e4-763a-4030-bb6f-4bc54ba3c27f",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1517941465255,
          "user_tz": 300,
          "elapsed": 250,
          "user": {
            "displayName": "Qi Hang",
            "photoUrl": "//lh3.googleusercontent.com/-jlaMUVkB1_4/AAAAAAAAAAI/AAAAAAAAACg/6DH9vg12aPs/s50-c-k-no/photo.jpg",
            "userId": "106529961477811677062"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Combine Retweet and text data, rank them, and convert them to utf-8\n",
        "retweets = [int(tweet['retweet_count']) for tweet in tweets]\n",
        "retweets_and_content = zip(tweets_text,retweets)\n",
        "s = sorted(retweets_and_content, key=lambda tup: tup[1], reverse=True)\n",
        "s = [(rt[0].encode('utf-8'), rt[1]) for rt in s]\n",
        "\n",
        "from prettytable import PrettyTable\n",
        "table = PrettyTable(['Tweet','Retweet Count'])\n",
        "for i in s[:10]:\n",
        "  table.add_row(i)\n",
        "table.align = 'l'\n",
        "print table\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------+\n",
            "| Tweet                                                                                                                                                                                                                                                                                | Retweet Count |\n",
            "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------+\n",
            "| Why would Kim Jong un insult me by calling me old when I would NEVER call him short and fat Oh well I try so hard to be his friend and maybe someday that will happen                                                                                                                | 268949        |\n",
            "| North Korean Leader Kim Jong Un just stated that the Nuclear Button is on his desk at all times Will someone from his depleted and food starved regime please inform him that I too have a Nuclear Button but it is a much bigger amp more powerful one than his and my Button works | 192676        |\n",
            "| The United States has foolishly given Pakistan more than 33 billion dollars in aid over the last 15 years and they have given us nothing but lies amp deceit thinking of our leaders as fools They give safe haven to the terrorists we hunt in Afghanistan with little help No more | 101319        |\n",
            "| I have great confidence in King Salman and the Crown Prince of Saudi Arabia they know exactly what they are doing                                                                                                                                                                    | 86544         |\n",
            "| Some of those they are harshly treating have been milking their country for years                                                                                                                                                                                                    | 82250         |\n",
            "| Now that the three basketball players are out of China and saved from years in jail LaVar Ball the father of LiAngelo is unaccepting of what I did for his son and that shoplifting is no big deal I should have left them in jail                                                   | 75246         |\n",
            "| My warmest condolences and sympathies to the victims and families of the terrible Las Vegas shooting God bless you                                                                                                                                                                   | 73594         |\n",
            "| Subject to the receipt of further information I will be allowing as President the long blocked and classified JFK FILES to be opened                                                                                                                                                 | 68599         |\n",
            "| In the East it could be the COLDEST New Year s Eve on record Perhaps we could use a little bit of that good old Global Warming that our Country but not other countries was going to pay TRILLIONS OF DOLLARS to protect against Bundle up                                           | 65493         |\n",
            "| Somebody please inform Jay Z that because of my policies Black Unemployment has just been reported to be at the LOWEST RATE EVER RECORDED                                                                                                                                            | 64295         |\n",
            "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "eWo0nuJwM7E1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**(3) Find the most popular Tweet Entities in your collection of tweets**\n",
        "\n",
        "Please plot the top 10 most-frequent hashtags and top 10 most-mentioned users in your collection of tweets."
      ]
    },
    {
      "metadata": {
        "id": "KuWmOKbVM7E4",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 1
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 546
        },
        "outputId": "86ccf690-63c8-4c61-e4a8-6667242cbbbe",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1517941469580,
          "user_tz": 300,
          "elapsed": 1882,
          "user": {
            "displayName": "Qi Hang",
            "photoUrl": "//lh3.googleusercontent.com/-jlaMUVkB1_4/AAAAAAAAAAI/AAAAAAAAACg/6DH9vg12aPs/s50-c-k-no/photo.jpg",
            "userId": "106529961477811677062"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Your code starts here\n",
        "#   Please add comments or text cells in between to explain the general idea of each block of the code.\n",
        "#   Please feel free to add more cells below this cell if necessary\n",
        "\n",
        "def extract_tweet_entities(statuses):\n",
        "    \n",
        "    # See https://dev.twitter.com/docs/tweet-entities for more details on tweet\n",
        "    # entities\n",
        "\n",
        "    if len(statuses) == 0:\n",
        "        return [], [], [], [], []\n",
        "    \n",
        "    screen_names = [ user_mention['screen_name'] \n",
        "                         for status in statuses\n",
        "                            for user_mention in status['entities']['user_mentions'] ]\n",
        "    \n",
        "    hashtags = [ hashtag['text'] \n",
        "                     for status in statuses \n",
        "                        for hashtag in status['entities']['hashtags'] ]\n",
        "\n",
        "    return screen_names, hashtags\n",
        "\n",
        "screen_names, hashtags = extract_tweet_entities(tweets)\n",
        "\n",
        "from collections import Counter\n",
        "# Explore the first five items for each...\n",
        "\n",
        "commonscreennames = zip(*Counter(screen_names).most_common()[0:20])\n",
        "commonhashtags = zip(*Counter(hashtags).most_common()[0:20])\n",
        "\n",
        "screennamesdata = Data([Bar(x=commonscreennames[0],y=commonscreennames[1])])\n",
        "layout = Layout(title = \"20 Most Frequent User Mentions\", xaxis = {'title':'User Mention'}, yaxis = {'title':'Frequency'})\n",
        "figure = Figure(data = screennamesdata,layout = layout)\n",
        "py.iplot(figure, image_width=800, image_height=600)\n",
        "\n",
        "#print json.dumps(Counter(urls).most_common()[0:5], indent=1)\n",
        "#print json.dumps(Counter(media).most_common()[0:5], indent=1)\n",
        "#print json.dumps(Counter(symbols).most_common()[0:5], indent=1)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~sevensail/16.embed\" height=\"525px\" width=\"100%\"></iframe>"
            ],
            "text/plain": [
              "<plotly.tools.PlotlyDisplay object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "6q88-9ZcS4Hl",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 1
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 546
        },
        "outputId": "df8a64c9-90bd-4c50-946f-8e555271c43c",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1517941476705,
          "user_tz": 300,
          "elapsed": 2443,
          "user": {
            "displayName": "Qi Hang",
            "photoUrl": "//lh3.googleusercontent.com/-jlaMUVkB1_4/AAAAAAAAAAI/AAAAAAAAACg/6DH9vg12aPs/s50-c-k-no/photo.jpg",
            "userId": "106529961477811677062"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "hashtagsdata = Data([Bar(x=commonhashtags[0],y=commonhashtags[1])])\n",
        "layout = Layout(title = \"20 Most Frequent Hashtags\", xaxis = {'title':'Hashtag'}, yaxis = {'title':'Frequency'})\n",
        "figure = Figure(data = hashtagsdata,layout = layout)\n",
        "py.iplot(figure, image_width=800, image_height=600)\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~sevensail/18.embed\" height=\"525px\" width=\"100%\"></iframe>"
            ],
            "text/plain": [
              "<plotly.tools.PlotlyDisplay object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "0KkPztTvM7FC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Plot** a histogram of the number of user mentions in the list using the following bins."
      ]
    },
    {
      "metadata": {
        "id": "0Z1SrWbEM7FV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        " ** (4) Getting \"All\" friends and \"All\" followers of a popular user in the tweets**"
      ]
    },
    {
      "metadata": {
        "id": "OphL55AZM7FY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "* choose a popular twitter user who has many followers in your collection of tweets.\n",
        "* Get the list of all friends and all followers of the twitter user.\n",
        "* Plot 20 out of the followers, plot their ID numbers and screen names in a table.\n",
        "* Plot 20 out of the friends (if the user has more than 20 friends), plot their ID numbers and screen names in a table."
      ]
    },
    {
      "metadata": {
        "id": "wcoSVx83M7Fa",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 1
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "9b0b4ed0-b25a-485a-c243-4cc812b15447",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1517941495728,
          "user_tz": 300,
          "elapsed": 6919,
          "user": {
            "displayName": "Qi Hang",
            "photoUrl": "//lh3.googleusercontent.com/-jlaMUVkB1_4/AAAAAAAAAAI/AAAAAAAAACg/6DH9vg12aPs/s50-c-k-no/photo.jpg",
            "userId": "106529961477811677062"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Your code starts here\n",
        "#   Please add comments or text cells in between to explain the general idea of each block of the code.\n",
        "#   Please feel free to add more cells below this cell if necessary\n",
        "\n",
        "\n",
        "from functools import partial\n",
        "from sys import maxint\n",
        "\n",
        "def get_friends_followers_ids(twitter_api, screen_name=None, user_id=None,\n",
        "                              friends_limit=maxint, followers_limit=maxint):\n",
        "    \n",
        "    # Must have either screen_name or user_id (logical xor)\n",
        "    assert (screen_name != None) != (user_id != None), \\\n",
        "    \"Must have screen_name or user_id, but not both\"\n",
        "    \n",
        "    # See https://dev.twitter.com/docs/api/1.1/get/friends/ids and\n",
        "    # https://dev.twitter.com/docs/api/1.1/get/followers/ids for details\n",
        "    # on API parameters\n",
        "    \n",
        "    get_friends_ids = partial(make_twitter_request, twitter_api.friends.ids, \n",
        "                              count=5000)\n",
        "    get_followers_ids = partial(make_twitter_request, twitter_api.followers.ids, \n",
        "                                count=5000)\n",
        "\n",
        "    friends_ids, followers_ids = [], []\n",
        "    \n",
        "    for twitter_api_func, limit, ids, label in [\n",
        "                    [get_friends_ids, friends_limit, friends_ids, \"friends\"], \n",
        "                    [get_followers_ids, followers_limit, followers_ids, \"followers\"]\n",
        "                ]:\n",
        "        \n",
        "        if limit == 0: continue\n",
        "        \n",
        "        cursor = -1\n",
        "        while cursor != 0:\n",
        "        \n",
        "            # Use make_twitter_request via the partially bound callable...\n",
        "            if screen_name: \n",
        "                response = twitter_api_func(screen_name=screen_name, cursor=cursor)\n",
        "            else: # user_id\n",
        "                response = twitter_api_func(user_id=user_id, cursor=cursor)\n",
        "\n",
        "            if response is not None:\n",
        "                ids += response['ids']\n",
        "                cursor = response['next_cursor']\n",
        "        \n",
        "            print >> sys.stderr, 'Fetched {0} total {1} ids for {2}'.format(len(ids), \n",
        "                                                    label, (user_id or screen_name))\n",
        "        \n",
        "            # XXX: You may want to store data during each iteration to provide an \n",
        "            # an additional layer of protection from exceptional circumstances\n",
        "        \n",
        "            if len(ids) >= limit or response is None:\n",
        "                break\n",
        "\n",
        "    # Do something useful with the IDs, like store them to disk...\n",
        "    friends = [(i,make_twitter_request(twitter_api.users.lookup, user_id = i)[0]['screen_name']) for i in friends_ids[:friends_limit]]\n",
        "    followers = [(i,make_twitter_request(twitter_api.users.lookup, user_id = i)[0]['screen_name']) for i in followers_ids[:followers_limit]]\n",
        "    return friends, followers\n",
        "\n",
        "# Sample usage\n",
        "\n",
        "twitter_api = oauth_login()\n",
        "\n",
        "friends, followers = get_friends_followers_ids(twitter_api, \n",
        "                                                       screen_name=\"realDonaldTrump\", \n",
        "                                                       friends_limit=20, \n",
        "                                                       followers_limit=20)\n",
        "\n",
        "#print friends\n",
        "#print followers"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fetched 45 total friends ids for realDonaldTrump\n",
            "Fetched 5000 total followers ids for realDonaldTrump\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "G2kvJtbpP76r",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 1
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 867
        },
        "outputId": "5d54825a-0b9a-4691-8bfc-a57fdd76f09d",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1517941496731,
          "user_tz": 300,
          "elapsed": 259,
          "user": {
            "displayName": "Qi Hang",
            "photoUrl": "//lh3.googleusercontent.com/-jlaMUVkB1_4/AAAAAAAAAAI/AAAAAAAAACg/6DH9vg12aPs/s50-c-k-no/photo.jpg",
            "userId": "106529961477811677062"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def PTable(columns,rows):\n",
        "   table = PrettyTable(columns)\n",
        "   for i in rows:\n",
        "    table.add_row(i)\n",
        "   table.align = 'l'\n",
        "   return table\n",
        "\n",
        "print(\"Friends\")\n",
        "print(PTable(['ID','Screen Name'],friends))\n",
        "\n",
        "print(\"Followers\")\n",
        "print(PTable(['ID','Screen Name'],followers))\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Friends\n",
            "+--------------------+-----------------+\n",
            "| ID                 | Screen Name     |\n",
            "+--------------------+-----------------+\n",
            "| 818927131883356161 | PressSec        |\n",
            "| 22703645           | TuckerCarlson   |\n",
            "| 56561449           | JesseBWatters   |\n",
            "| 822215673812119553 | WhiteHouse      |\n",
            "| 823367015830323201 | Scavino45       |\n",
            "| 471672239          | KellyannePolls  |\n",
            "| 20733972           | Reince          |\n",
            "| 322293052          | RealRomaDowney  |\n",
            "| 720293443260456960 | Trump           |\n",
            "| 2325495378         | TrumpGolf       |\n",
            "| 245963716          | TiffanyATrump   |\n",
            "| 50769180           | IngrahamAngle   |\n",
            "| 22203756           | mike_pence      |\n",
            "| 729676086632656900 | TeamTrump       |\n",
            "| 14669951           | DRUDGE_REPORT   |\n",
            "| 475802156          | MrsVanessaTrump |\n",
            "| 75541946           | LaraLeaTrump    |\n",
            "| 41634520           | seanhannity     |\n",
            "| 37764422           | foxnation       |\n",
            "| 4121225056         | CLewandowski_   |\n",
            "+--------------------+-----------------+\n",
            "Followers\n",
            "+--------------------+-----------------+\n",
            "| ID                 | Screen Name     |\n",
            "+--------------------+-----------------+\n",
            "| 960942264880001024 | miki83427437    |\n",
            "| 960941884083490816 | PhilipDeangeli4 |\n",
            "| 3293847915         | pepianoxiv      |\n",
            "| 960937730665480193 | CarolRenaud3    |\n",
            "| 960938318597894145 | KeeganBoer      |\n",
            "| 960942287189696512 | hgjghg7         |\n",
            "| 3016602334         | gerson_mosses   |\n",
            "| 960941811890974720 | AgustianSyahpu6 |\n",
            "| 960941278421766145 | paigecross1404  |\n",
            "| 960941567253950464 | bqvnsweis305    |\n",
            "| 612661113          | Future_MrsKnapp |\n",
            "| 2593576722         | Lambda_Jamz     |\n",
            "| 960941968619487232 | tsoka_james     |\n",
            "| 960866050417061888 | osteopathinkels |\n",
            "| 921544023575285761 | Vctor35809639   |\n",
            "| 959877704509263873 | Long96092153    |\n",
            "| 960941727019237376 | clotildecliff13 |\n",
            "| 960938573221519360 | Ehruns2         |\n",
            "| 959790802590486533 | MasangoEdgar    |\n",
            "| 959136354339876864 | Anasskharsahot1 |\n",
            "+--------------------+-----------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "w786jRWbM7Fk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# The Solution: implement a data science solution to the problem you are trying to solve."
      ]
    },
    {
      "metadata": {
        "id": "cVsTpmpAM7Fm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Briefly describe the idea of your solution to the problem in the following cell:"
      ]
    },
    {
      "metadata": {
        "id": "92n9UWNAWudx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "First, we did sentiment analysis on extracted trump's tweets using Textblob. Then, we downloaded stock values data using quandl, another python api. Based on the ploarity we got from sentiment analysis and the stock values, we try to find some relationship between them using correlation matrix."
      ]
    },
    {
      "metadata": {
        "id": "YP--gUw1M7Fo",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import time\n",
        "import datetime\n",
        "import pandas\n",
        "import quandl\n",
        "\n",
        "quandl.ApiConfig.api_key = 'uJJsbxupJZuBAG5CtjKH'\n",
        " \n",
        "#using quandl to download stock values, give a start_date, end_date, and stock name\n",
        "#it will return a pandas' matrix\n",
        "#doc : https://blog.quandl.com/getting-started-with-the-quandl-api\n",
        "#code example : https://chrisconlan.com/download-historical-stock-data-google-r-python/\n",
        "#\n",
        "#someone said Google Finance API was to shut down on October 2012.\n",
        "#https://stackoverflow.com/questions/46070126/google-finance-json-stock-quote-stopped-working/\n",
        "def quandl_stocks(symbol, start_date=(2017, 1, 1), end_date=None, returns='pandas'):\n",
        "    \"\"\"\n",
        "    symbol is a string representing a stock symbol, e.g. 'AAPL'\n",
        " \n",
        "    start_date and end_date are tuples of integers representing the year, month,\n",
        "    and day\n",
        " \n",
        "    end_date defaults to the current date when None\n",
        "    \"\"\"\n",
        " \n",
        "    query_list = ['WIKI' + '/' + symbol + '.' + str(1), 'WIKI' + '/' + symbol + '.' + str(4)]\n",
        " \n",
        "    start_date = datetime.date(*start_date)\n",
        " \n",
        "    if end_date:\n",
        "        end_date = datetime.date(*end_date)\n",
        "    else:\n",
        "        end_date = datetime.date.today()\n",
        "         \n",
        "    return quandl.get(query_list, \n",
        "            returns='pandas', \n",
        "            start_date=start_date,\n",
        "            end_date=end_date,\n",
        "            collapse='daily',\n",
        "            order='asc'\n",
        "            )\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EqO783eDM7Fu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Write codes to implement the solution in python:"
      ]
    },
    {
      "metadata": {
        "id": "J00kO1aJM7F3",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import textblob\n",
        "import datetime\n",
        "\n",
        "def analize_sentiment(tweet):\n",
        "    '''\n",
        "    Utility function to classify the polarity of a tweet\n",
        "    using textblob.\n",
        "    '''\n",
        "    analysis = textblob.TextBlob(tweet)\n",
        "    return analysis.sentiment.polarity\n",
        "  \n",
        "\n",
        "#sentiment_data = [analize_sentiment(tweetText) for tweetText in tweets_text]\n",
        "#pos_tweets = [ (index, tweet) for index, tweet in enumerate(tweets) if sentiment_data[index] > 0]\n",
        "#neu_tweets = [ (index, tweet) for index, tweet in enumerate(tweets) if sentiment_data[index] == 0]\n",
        "#neg_tweets = [ (index, tweet) for index, tweet in enumerate(tweets) if sentiment_data[index] < 0]\n",
        "#pos_tweets_dict = dict(pos_tweets)\n",
        "#neu_tweets_dict = dict(neu_tweets)\n",
        "#neg_tweets_dict = dict(neg_tweets)\n",
        "#print(\"Percentage of positive tweets: {}%\".format(len(pos_tweets)*100/len(tweets)))\n",
        "#print(\"Percentage of neutral tweets: {}%\".format(len(neu_tweets)*100/len(tweets)))\n",
        "#print(\"Percentage de negative tweets: {}%\".format(len(neg_tweets)*100/len(tweets)))\n",
        "\n",
        "import datetime\n",
        "from dateutil import parser\n",
        "import pandas as pd\n",
        "\n",
        "def getDailyChanges(ticker, tweetsPd):    \n",
        "  #get stock values from quandl, and select the 0 and 3 lines which is open and close values\n",
        "  ticker_data = quandl_stocks(ticker, (2016, 9, 1))\n",
        "  #open subtract close values, and we get a daily change values\n",
        "  ticker_data_bri = ticker_data['WIKI/' + ticker + ' - Open'].sub(ticker_data['WIKI/' + ticker + ' - Close'], axis = 0).div(ticker_data['WIKI/' + ticker + ' - Open'], axis = 0)\n",
        "  #convert it to dataframe\n",
        "  ticker_data_bri = pd.DataFrame(ticker_data_bri)\n",
        "  #join them together and get the correlation matrix\n",
        "  resultPd = tweetsPd.join(ticker_data_bri, how = 'inner')\n",
        "  return resultPd\n",
        "\n",
        "def getDailyPrice(ticker, tweetsPd):\n",
        "  #get stock values from quandl, and select the 0 and 3 lines which is open and close values\n",
        "  ticker_data = quandl_stocks(ticker, (2017, 9, 1))\n",
        "  #open subtract close values, and we get a daily change values\n",
        "  ticker_data_bri = ticker_data['WIKI/' + ticker + ' - Open']\n",
        "  #convert it to dataframe\n",
        "  ticker_data_bri = pd.DataFrame(ticker_data_bri)\n",
        "  resultPd = tweetsPd.join(ticker_data_bri, how = 'inner')\n",
        "  return resultPd\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "c5ShwJDPu2Lb",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 5
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "39ba9a9f-1fba-4389-8210-43f26af73128",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1517941530102,
          "user_tz": 300,
          "elapsed": 8629,
          "user": {
            "displayName": "Qi Hang",
            "photoUrl": "//lh3.googleusercontent.com/-jlaMUVkB1_4/AAAAAAAAAAI/AAAAAAAAACg/6DH9vg12aPs/s50-c-k-no/photo.jpg",
            "userId": "106529961477811677062"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# RUNNING SENTIMENT API && MASSAGING DATA\n",
        "sentiment_data = [analize_sentiment(tweetText) for tweetText in tweets_text]\n",
        "#tweets_date is a list, contains tweets' created date in '%Y-%m-%d' format\n",
        "tweets_date = [(parser.parse(tweetText['created_at'])).strftime('%Y-%m-%d') for tweetText in tweets]\n",
        "\n",
        "# Create a dictionary of (tweet date):sentiment such that tweets dated from \n",
        "# the same day will contribute to a weighted average of a single dictionary value\n",
        "# this value is output as tweetsPd\n",
        "tweets_plo_key_date = {}\n",
        "for i in range(len(tweets_date)):\n",
        "  if tweets_date[i] in tweets_plo_key_date:\n",
        "    #below, add all polarity in the same date\n",
        "    tweets_plo_key_date[tweets_date[i]] = (tweets_plo_key_date[tweets_date[i]][0] + sentiment_data[i],tweets_plo_key_date[tweets_date[i]][1]+1)\n",
        "  else:\n",
        "    tweets_plo_key_date[tweets_date[i]] = (sentiment_data[i],1)\n",
        "tweets_plo_key_date1 = {}\n",
        "for i in tweets_plo_key_date:\n",
        "  tweets_plo_key_date1[i] = float(tweets_plo_key_date[i][0])/tweets_plo_key_date[i][1]\n",
        "\n",
        "tweetsPd = pd.DataFrame(tweets_plo_key_date1.items(), columns=['Date', 'DateValue']).set_index('Date').sort_index()\n",
        "\n",
        "# EXTRACTING Plottable DATA\n",
        "tickers   = ['XOM', 'AAPL', 'GEO', 'HCA']\n",
        "deltaTick = []\n",
        "corrs     = []\n",
        "prices = []\n",
        "for ticker in tickers:\n",
        "  print ticker\n",
        "  changes = getDailyChanges(ticker, tweetsPd)\n",
        "  pri = getDailyPrice(ticker, tweetsPd)\n",
        "  prices.append(pri)\n",
        "  deltaTick.append(changes)\n",
        "  corrs.append(changes.corr()['DateValue'][0])\n",
        "  \n",
        "print corrs"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "XOM\n",
            "AAPL\n",
            "GEO\n",
            "HCA\n",
            "[-0.007795146384413103, 0.103357078112181, -0.06436174961034623, 0.007543110270724468]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4VhRQlt-KItM",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 1
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 546
        },
        "outputId": "73d0be40-7f46-4b42-b8b9-9d1d7d805ce2",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1517941647564,
          "user_tz": 300,
          "elapsed": 2119,
          "user": {
            "displayName": "Qi Hang",
            "photoUrl": "//lh3.googleusercontent.com/-jlaMUVkB1_4/AAAAAAAAAAI/AAAAAAAAACg/6DH9vg12aPs/s50-c-k-no/photo.jpg",
            "userId": "106529961477811677062"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import plotly.plotly as py\n",
        "import plotly.graph_objs as go\n",
        "from plotly.graph_objs import *\n",
        "import numpy as np\n",
        "\n",
        "plots = []\n",
        "for i in range(len(tickers)):\n",
        "  data = list(deltaTick[i][0])\n",
        "  trace = go.Box(y=data,name=tickers[i])\n",
        "  plots.append(trace)\n",
        "  \n",
        "  \n",
        "layout = Layout(title = \"Box and Whisker Plots of Selected Stocks on Days that Trump Tweeted\", yaxis = {'title':'% change (stock price)'})\n",
        "figure = Figure(data = plots,layout = layout)\n",
        "py.iplot(figure)\n",
        "  "
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~sevensail/22.embed\" height=\"525px\" width=\"100%\"></iframe>"
            ],
            "text/plain": [
              "<plotly.tools.PlotlyDisplay object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "metadata": {
        "id": "qT1Ntu1N84MO",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 1
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 546
        },
        "outputId": "8d0d8538-66cf-4a55-bab0-405697fc1214",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1517808769950,
          "user_tz": 300,
          "elapsed": 1948,
          "user": {
            "displayName": "Qi Hang",
            "photoUrl": "//lh3.googleusercontent.com/-jlaMUVkB1_4/AAAAAAAAAAI/AAAAAAAAACg/6DH9vg12aPs/s50-c-k-no/photo.jpg",
            "userId": "106529961477811677062"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Linear Regression Comparison (bar charts)\n",
        "import plotly.plotly as py\n",
        "import plotly.graph_objs as go\n",
        "\n",
        "data = [go.Bar(\n",
        "            x=tickers,\n",
        "            y=corrs,\n",
        "            name=tickers\n",
        "    )]\n",
        "\n",
        "layout = Layout(title = \"Bar Chart Comparison Regression Values of Selected Stocks.  (Tweet Sentiment vs. Stock %Change)\", yaxis = {'title':'R-Value'})\n",
        "figure = Figure(data = data,layout = layout)\n",
        "py.iplot(figure)\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~GunnarHorve/197.embed\" height=\"525px\" width=\"100%\"></iframe>"
            ],
            "text/plain": [
              "<plotly.tools.PlotlyDisplay object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "metadata": {
        "id": "50rqpUB4M7F-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Results: summarize and visualize the results discovered from the analysis\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "4qb-TO06YUvJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Chart below reflects how Trump's tweets influence the stock price of Apple Inc. We chose Apple Inc. because Trump's taxes plan. The taxes plan means companies holding lots of cash abroad such as Apple (AAPL - Get Report) would finally bring their money back to the U.S.\n",
        "\n",
        "The blue line indicates how the stock price fluctuate and the orange line indicates the polarity of Trump's tweets."
      ]
    },
    {
      "metadata": {
        "id": "hX68-fPaWr4q",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 1
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 546
        },
        "outputId": "62432532-061b-4ae3-acef-273204ec7e70",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1517942240384,
          "user_tz": 300,
          "elapsed": 2050,
          "user": {
            "displayName": "Qi Hang",
            "photoUrl": "//lh3.googleusercontent.com/-jlaMUVkB1_4/AAAAAAAAAAI/AAAAAAAAACg/6DH9vg12aPs/s50-c-k-no/photo.jpg",
            "userId": "106529961477811677062"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "aaplprice = list(deltaTick[1][0])\n",
        "aaplpol = list(deltaTick[1]['DateValue'])\n",
        "\n",
        "trace1 = go.Scatter(\n",
        "    x = prices[1].index,\n",
        "    y = aaplprice,\n",
        "    name='stock price',\n",
        "    mode = 'lines'\n",
        ")\n",
        "trace2 = go.Scatter(\n",
        "    x = prices[1].index,\n",
        "    y = aaplpol,\n",
        "    name='tweets polarity',\n",
        "    yaxis='y2',\n",
        "    mode = 'lines+markers'\n",
        ")\n",
        "data = [trace1, trace2]\n",
        "layout = go.Layout(\n",
        "    title='Line Chart with polarity and stock price',\n",
        "    yaxis=dict(\n",
        "        title='stock price change %',\n",
        "        range=[-0.05, 0.04]\n",
        "    ),\n",
        "    yaxis2=dict(\n",
        "        title='tweets polarity',\n",
        "        titlefont=dict(\n",
        "            color='rgb(148, 103, 189)'\n",
        "        ),\n",
        "        tickfont=dict(\n",
        "            color='rgb(148, 103, 189)'\n",
        "        ),\n",
        "        overlaying='y',\n",
        "        side='right',\n",
        "        range=[-0.6,2]\n",
        "    )\n",
        ")\n",
        "fig = go.Figure(data=data, layout=layout)\n",
        "py.iplot(fig, filename='Line Chart with polarity and stock price')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~sevensail/8.embed\" height=\"525px\" width=\"100%\"></iframe>"
            ],
            "text/plain": [
              "<plotly.tools.PlotlyDisplay object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "metadata": {
        "id": "ibJ8z3S5gWmh",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 1
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 546
        },
        "outputId": "ca08fb5e-d68c-4fe2-b433-2563e2819b62",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1517944493335,
          "user_tz": 300,
          "elapsed": 1985,
          "user": {
            "displayName": "Qi Hang",
            "photoUrl": "//lh3.googleusercontent.com/-jlaMUVkB1_4/AAAAAAAAAAI/AAAAAAAAACg/6DH9vg12aPs/s50-c-k-no/photo.jpg",
            "userId": "106529961477811677062"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "aaplprice = list(deltaTick[1][0])\n",
        "aaplpol = list(deltaTick[1]['DateValue'])\n",
        "aaplCorCounter = 0\n",
        "aaplCorCounterp = 0;\n",
        "for i in range(len(aaplpol)):\n",
        "  if (aaplprice[i]*(aaplpol[i]))>=0:\n",
        "    aaplCorCounterp+=1\n",
        "  aaplCorCounter+=1;\n",
        "\n",
        "labels = ['Negative','Positive']\n",
        "values = [aaplCorCounter - aaplCorCounterp, aaplCorCounterp]\n",
        "\n",
        "trace = go.Pie(labels=labels, values=values)\n",
        "\n",
        "py.iplot([trace], filename='basic_pie_chart')"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~sevensail/30.embed\" height=\"525px\" width=\"100%\"></iframe>"
            ],
            "text/plain": [
              "<plotly.tools.PlotlyDisplay object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "metadata": {
        "id": "nrSFrDsTa91k",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The scatter plot shows the relationship between Trump's twwet polarity and Apple Inc. stock price. As we can see, it's not a strong correlation. Stock price is related to many other factors."
      ]
    },
    {
      "metadata": {
        "id": "TeHUst5TL_pV",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 1
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 546
        },
        "outputId": "3c621915-d157-4232-d2c3-45829c80d974",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1517941699602,
          "user_tz": 300,
          "elapsed": 3852,
          "user": {
            "displayName": "Qi Hang",
            "photoUrl": "//lh3.googleusercontent.com/-jlaMUVkB1_4/AAAAAAAAAAI/AAAAAAAAACg/6DH9vg12aPs/s50-c-k-no/photo.jpg",
            "userId": "106529961477811677062"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "aapldeltastock = list(deltaTick[1][0])\n",
        "aaplpolarity = list(deltaTick[1]['DateValue'])\n",
        "scatterplot = [Scatter(x = aapldeltastock, y = aaplpolarity, mode = 'markers')]\n",
        "layout = Layout(title = \"Scatter Plot for AAPL - Price Difference versus Polarity\", xaxis = {'title':'Price Difference'}, yaxis = {'title':'Polarity'})\n",
        "figure = Figure(data = scatterplot, layout = layout)\n",
        "py.iplot(figure)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~sevensail/26.embed\" height=\"525px\" width=\"100%\"></iframe>"
            ],
            "text/plain": [
              "<plotly.tools.PlotlyDisplay object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "metadata": {
        "id": "BEUX2SxZM7GG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "*-----------------\n",
        "# Done\n",
        "\n",
        "All set! \n",
        "\n",
        "** What do you need to submit?**\n",
        "\n",
        "* **Notebook File**: Save this Jupyter notebook, and find the notebook file in your folder (for example, \"filename.ipynb\"). This is the file you need to submit. Please make sure all the plotted tables and figures are in the notebook. If you used \"jupyter notebook --pylab=inline\" to open the notebook, all the figures and tables should have shown up in the notebook.\n",
        "\n",
        "* **PPT Slides**: please prepare PPT slides (for 10 minutes' talk) to present about the case study . Each team present their case studies in class for 10 minutes.\n",
        "\n",
        "Please compress all the files in a zipped file.\n",
        "\n",
        "\n",
        "** How to submit: **\n",
        "\n",
        "        Please submit through Canvas, in the Assignment \"Case Study 1\".\n",
        "        \n",
        "** Note: Each team only needs to submit one submission in Canvas **"
      ]
    },
    {
      "metadata": {
        "id": "9o0WRMYPM7GI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Peer-Review Grading Template:\n",
        "\n",
        "** Total Points: (100 points) ** Please don't worry about the absolute scores, we will rescale the final grading according to the performance of all teams in the class.\n",
        "\n",
        "Please add an \"**X**\" mark in front of your rating: \n",
        "\n",
        "For example:\n",
        "\n",
        "*2: bad*\n",
        "          \n",
        "**X** *3: good*\n",
        "    \n",
        "*4: perfect*\n",
        "\n",
        "\n",
        "    ---------------------------------\n",
        "    The Problem: \n",
        "    ---------------------------------\n",
        "    \n",
        "    1. (5 points) how well did the team describe the problem they are trying to solve using twitter data? \n",
        "       0: not clear\n",
        "       1: I can barely understand the problem\n",
        "       2: okay, can be improved\n",
        "       3: good, but can be improved\n",
        "       4: very good\n",
        "       5: crystal clear\n",
        "    \n",
        "    2. (10 points) do you think the problem is important or has a potential impact?\n",
        "        0: not important at all\n",
        "        2: not sure if it is important\n",
        "        4: seems important, but not clear\n",
        "        6: interesting problem\n",
        "        8: an important problem, which I want to know the answer myself\n",
        "       10: very important, I would be happy invest money on a project like this.\n",
        "    \n",
        "    ----------------------------------\n",
        "    Data Collection:\n",
        "    ----------------------------------\n",
        "    \n",
        "    3. (10 points) Do you think the data collected are relevant and sufficient for solving the above problem? \n",
        "       0: not clear\n",
        "       2: I can barely understand what data they are trying to collect\n",
        "       4: I can barely understand why the data is relevant to the problem\n",
        "       6: the data are relevant to the problem, but better data can be collected\n",
        "       8: the data collected are relevant and at a proper scale (> 300 tweets)\n",
        "      10: the data are properly collected and they are sufficient\n",
        "\n",
        "    -----------------------------------\n",
        "    Data Exploration:\n",
        "    -----------------------------------\n",
        "    4. How well did the team solve the following task:\n",
        "    (1) Word Count (5 points):\n",
        "       0: missing answer\n",
        "       1: okay, but with major problems\n",
        "       3: good, but with minor problems\n",
        "       5: perfect\n",
        "    \n",
        "    (2) Find the most popular tweets in your collection of tweets: (5 points)\n",
        "       0: missing answer\n",
        "       1: okay, but with major problems\n",
        "       3: good, but with minor problems\n",
        "       5: perfect\n",
        "    \n",
        "    (3) Find popular twitter entities  (5 points)\n",
        "       0: missing answer\n",
        "       1: okay, but with major problems\n",
        "       3: good, but with minor problems\n",
        "       5: perfect\n",
        "\n",
        "    (4) Find user's followers and friends (5 points)\n",
        "       0: missing answer\n",
        "       1: okay, but with major problems\n",
        "       3: good, but with minor problems\n",
        "       5: perfect\n",
        "\n",
        "    -----------------------------------\n",
        "    The Solution\n",
        "    -----------------------------------\n",
        "    5.  how well did the team describe the solution they used to solve the problem? \n",
        "       0: not clear\n",
        "       2: I can barely understand\n",
        "       4: okay, can be improved\n",
        "       6: good, but can be improved\n",
        "       8: very good\n",
        "       10: crystal clear\n",
        "       \n",
        "    6. how well is the solution in solving the problem? \n",
        "       0: not relevant\n",
        "       1: barely relevant to the problem\n",
        "       2: okay solution, but there is an easier solution.\n",
        "       3: good, but can be improved\n",
        "       4: very good, but solution is simple/old\n",
        "       5: innovative and technically sound\n",
        "       \n",
        "    7. how well did the team implement the solution in python? \n",
        "       0: the code is not relevant to the solution proposed\n",
        "       2: the code is barely understandable, but not relevant\n",
        "       4: okay, the code is clear but incorrect\n",
        "       6: good, the code is correct, but with major errors\n",
        "       8: very good, the code is correct, but with minor errors\n",
        "      10: perfect \n",
        "   \n",
        "    -----------------------------------\n",
        "    The Results\n",
        "    -----------------------------------\n",
        "     8.  How well did the team present the results they found in the data? \n",
        "       0: not clear\n",
        "       2: I can barely understand\n",
        "       4: okay, can be improved\n",
        "       6: good, but can be improved\n",
        "       8: very good\n",
        "      10: crystal clear\n",
        "       \n",
        "     9.  How do you think the results they found in the data? \n",
        "       0: not clear\n",
        "       1: likely to be wrong\n",
        "       2: okay, maybe wrong\n",
        "       3: good, but can be improved\n",
        "       4: make sense, but not interesting\n",
        "       5: make sense and very interesting\n",
        "     \n",
        "    -----------------------------------\n",
        "    The Presentation\n",
        "    -----------------------------------\n",
        "    10. How all the different parts (data, problem, solution, result) fit together as a coherent story?  \n",
        "       0: they are irrelevant\n",
        "       1: I can barely understand how they are related to each other\n",
        "       2: okay, the problem is good, but the solution doesn't match well, or the problem is not solvable.\n",
        "       3: good, but the results don't make much sense in the context\n",
        "       4: very good fit, but not exciting (the storyline can be improved/polished)\n",
        "       5: a perfect story\n",
        "      \n",
        "    11. Did the presenter make good use of the 10 minutes for presentation?  \n",
        "       0: the team didn't present\n",
        "       1: bad, barely finished a small part of the talk\n",
        "       2: okay, barely finished most parts of the talk.\n",
        "       3: good, finished all parts of the talk, but some part is rushed\n",
        "       4: very good, but the allocation of time on different parts can be improved.\n",
        "       5: perfect timing and good use of time      \n",
        "\n",
        "    12. How well do you think of the presentation (overall quality)?  \n",
        "       0: the team didn't present\n",
        "       1: bad\n",
        "       2: okay\n",
        "       3: good\n",
        "       4: very good\n",
        "       5: perfect\n",
        "\n",
        "\n",
        "    -----------------------------------\n",
        "    Overall: \n",
        "    -----------------------------------\n",
        "    13. How many points out of the 100 do you give to this project in total?  Please don't worry about the absolute scores, we will rescale the final grading according to the performance of all teams in the class.\n",
        "    Total score:\n",
        "    \n",
        "    14. What are the strengths of this project? Briefly, list up to 3 strengths.\n",
        "       1: \n",
        "       2:\n",
        "       3:\n",
        "    \n",
        "    15. What are the weaknesses of this project? Briefly, list up to 3 weaknesses.\n",
        "       1:\n",
        "       2:\n",
        "       3:\n",
        "    \n",
        "    16. Detailed comments and suggestions. What suggestions do you have for this project to improve its quality further.\n",
        "    \n",
        "    \n",
        "    \n",
        "\n",
        "    ---------------------------------\n",
        "    Your Vote: \n",
        "    ---------------------------------\n",
        "    1. [Overall Quality] Between the two submissions that you are reviewing, which team would you vote for a better score?  \n",
        "       -1: I vote the other team is better than this team\n",
        "        0: the same\n",
        "        1: I vote this team is better than the other team \n",
        "        \n",
        "    2. [Presentation] Among all the teams in the presentation, which team do you think deserves the best presentation award for this case study?  \n",
        "        1: Team 1\n",
        "        2: Team 2\n",
        "        3: Team 3\n",
        "        4: Team 4\n",
        "        5: Team 5\n",
        "        6: Team 6\n",
        "        7: Team 7\n",
        "        8: Team 8\n",
        "        9: Team 9\n",
        "       10: Team 10\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "XW-vIEL_M7GI",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}